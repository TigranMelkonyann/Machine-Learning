{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Softmax function\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cross_entropy_loss(y, probs):\n",
    "    m = y.shape[0]\n",
    "    log_likelihood = -np.log(probs[range(m), y])\n",
    "    return np.sum(log_likelihood) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient of the cost function\n",
    "def compute_gradient(X, y, probs):\n",
    "    m = X.shape[0]\n",
    "    grad = np.dot(X.T, (probs - np.eye(np.max(y) + 1)[y])) / m\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Batch Gradient Descent with Early Stopping\n",
    "def softmax_regression(X_train, y_train, X_val, y_val, learning_rate=0.01, max_epochs=1000, patience=5):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    n_features = X_train.shape[1]\n",
    "    theta = np.zeros((n_features, n_classes))  # Initialize parameters\n",
    "\n",
    "    best_theta = theta\n",
    "    best_val_loss = np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Compute probabilities\n",
    "        logits = np.dot(X_train, theta)\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # Compute gradient and update parameters\n",
    "        grad = compute_gradient(X_train, y_train, probs)\n",
    "        theta -= learning_rate * grad\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_logits = np.dot(X_val, theta)\n",
    "        val_probs = softmax(val_logits)\n",
    "        val_loss = cross_entropy_loss(y_val, val_probs)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_theta = theta\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            train_loss = cross_entropy_loss(y_train, probs)\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    return best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 1.0986, Val Loss = 0.9975\n",
      "Epoch 100: Train Loss = 0.3926, Val Loss = 0.3904\n",
      "Epoch 200: Train Loss = 0.3599, Val Loss = 0.3707\n",
      "Epoch 300: Train Loss = 0.3451, Val Loss = 0.3606\n",
      "Epoch 400: Train Loss = 0.3365, Val Loss = 0.3546\n",
      "Epoch 500: Train Loss = 0.3309, Val Loss = 0.3508\n",
      "Epoch 600: Train Loss = 0.3270, Val Loss = 0.3482\n",
      "Epoch 700: Train Loss = 0.3240, Val Loss = 0.3463\n",
      "Epoch 800: Train Loss = 0.3216, Val Loss = 0.3449\n",
      "Epoch 900: Train Loss = 0.3197, Val Loss = 0.3438\n",
      "Validation Accuracy: 86.67%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "theta = softmax_regression(X_train, y_train, X_val, y_val, learning_rate=0.1, max_epochs=1000, patience=5)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "def predict(X, theta):\n",
    "    logits = np.dot(X, theta)\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "y_pred = predict(X_val, theta)\n",
    "accuracy = np.mean(y_pred == y_val)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
